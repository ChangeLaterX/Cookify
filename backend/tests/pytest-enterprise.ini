# ============================================================================
# üè¢ TEST CONFIGURATION
# ============================================================================
# Professional test configuration for medium-sized software company
# Cookify Backend - Quality Assurance Standards

[pytest]
# Core Configuration
minversion = 7.0
python_files = test_*.py *_test.py tests_*.py
python_classes = Test* *Tests
python_functions = test_*
testpaths = tests

# Grade test execution
addopts = 
    --strict-markers
    --strict-config
    --tb=short
    --verbose
    --durations=20
    --durations-min=1.0
    --maxfail=10
    --tb=auto
    --color=yes
    --code-highlight=yes

# Async support
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Parallel execution
# Use with: pytest -n auto
# Requires: pip install pytest-xdist

# Coverage configuration  
# Use with: pytest --cov
# Requires: pip install pytest-cov

# ============================================================================
# üè∑Ô∏è TEST MARKERS -  CLASSIFICATION
# ============================================================================
markers =
    # Test Types
    unit: Unit tests - fast, isolated, no external dependencies
    integration: Integration tests - database, API, external services
    e2e: End-to-end tests - full system testing
    smoke: Smoke tests - basic functionality verification
    regression: Regression tests - prevent known bugs from returning
    
    # Performance Categories
    performance: Performance and load tests
    benchmark: Benchmark tests with timing requirements
    stress: Stress tests for system limits
    memory: Memory usage and leak tests
    
    # Domain Classifications
    auth: Authentication and authorization domain
    ingredients: Ingredients management domain
    ocr: OCR and image processing domain
    api: API endpoint tests
    database: Database operation tests
    
    # Security Classifications
    security: Security-focused tests
    vulnerability: Vulnerability and penetration tests
    compliance: Compliance and audit tests
    
    # Environment Categories
    ci_safe: Safe to run in CI environment
    local_only: Only run in local development
    staging_only: Only run in staging environment
    production_safe: Safe to run against production-like data
    
    # Infrastructure Categories
    requires_tesseract: Requires Tesseract OCR installation
    requires_gpu: Requires GPU acceleration
    requires_network: Requires network connectivity
    requires_docker: Requires Docker environment
    
    # Test Qualities
    slow: Slow tests (>30 seconds)
    flaky: Known flaky tests (investigate and fix)
    experimental: Experimental features under development
    deprecated: Deprecated functionality (plan for removal)
    
    # Business Categories
    critical: Critical business functionality
    feature: New feature tests
    bugfix: Bug fix verification tests
    enhancement: Enhancement and improvement tests

# ============================================================================
# üîß FILTERING EXAMPLES
# ============================================================================
# Run only unit tests:           pytest -m "unit"
# Run critical functionality:    pytest -m "critical"
# Skip slow tests:              pytest -m "not slow"
# CI-safe tests only:           pytest -m "ci_safe"
# Domain-specific testing:      pytest -m "auth and unit"
# Performance suite:            pytest -m "performance or benchmark"
# Security audit:               pytest -m "security or vulnerability"
# Pre-production check:         pytest -m "critical and not experimental"

# ============================================================================
# ‚ö†Ô∏è WARNING FILTERS - PRODUCTION READY
# ============================================================================
filterwarnings =
    # Ignore common third-party warnings
    ignore::UserWarning:requests.*
    ignore::DeprecationWarning:cryptography.*
    ignore::DeprecationWarning:pytest.*
    ignore::pytest.PytestUnraisableExceptionWarning
    
    # Ignore async-related warnings
    ignore::RuntimeWarning:asyncio.*
    ignore::ResourceWarning:asyncio.*
    
    # Ignore SQL-related warnings in tests
    ignore::sqlalchemy.exc.SAWarning
    
    # Convert important warnings to errors
    error::UserWarning:cookify.*
    error::DeprecationWarning:cookify.*

# ============================================================================
# üìä LOGGING CONFIGURATION
# ============================================================================
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# ============================================================================
# üéØ QUALITY THRESHOLDS
# ============================================================================
# These can be overridden via environment variables or command line

# Coverage thresholds (use with --cov-fail-under)
# Unit tests: 90%+
# Integration tests: 75%+
# Overall: 80%+

# Performance thresholds
# API response time: < 200ms (P95)
# Database queries: < 100ms (P95)
# OCR processing: < 30s per image
# Health checks: < 50ms

# Memory thresholds
# Memory growth: < 10% per hour
# Memory leaks: 0 tolerance
# Peak memory: < 512MB per process

# ============================================================================
# üèóÔ∏è PARALLEL EXECUTION SETTINGS
# ============================================================================
# For CI environments with multiple cores
# Use: pytest -n auto --dist worksteal

# Distribution strategies:
# - loadscope: Distribute by test scope (fastest for most cases)
# - loadfile: Distribute by test file
# - worksteal: Dynamic work stealing (best for uneven test durations)

# ============================================================================
# üìà REPORTING CONFIGURATION  
# ============================================================================
# HTML Report: pytest --html=report.html --self-contained-html
# JSON Report: pytest --json-report --json-report-file=report.json
# JUnit XML: pytest --junitxml=junit.xml
# Coverage HTML: pytest --cov --cov-report=html
# Coverage XML: pytest --cov --cov-report=xml

# ============================================================================
# üîí SECURITY CONSIDERATIONS
# ============================================================================
# - Never commit real credentials to test files
# - Use environment variables for sensitive data
# - Sanitize logs in CI/CD pipelines
# - Use separate test databases
# - Mock external services in unit tests
# - Validate all test data is synthetic

# ============================================================================
# üìù  BEST PRACTICES
# ============================================================================
# 1. Test Naming: test_[component]_[scenario]_[expected_result]
# 2. Test Structure: Arrange, Act, Assert pattern
# 3. Test Independence: Tests should not depend on each other
# 4. Test Data: Use factories/fixtures for consistent test data
# 5. Error Testing: Always test error conditions
# 6. Documentation: Document complex test logic
# 7. Performance: Keep unit tests under 100ms each
# 8. Cleanup: Always clean up resources (use fixtures)

# ============================================================================
# üìö DOCUMENTATION LINKS
# ============================================================================
# - pytest docs: https://docs.pytest.org/
# - Company test standards: [internal wiki link]
# - Test data management: [internal docs]
# - CI/CD pipeline docs: [internal docs]
# - Security testing guide: [internal docs]
