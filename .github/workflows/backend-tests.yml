name: Backend API Tests

on:
  push:
    branches: [ main, dev ]
    paths:
      - 'backend/**'
      - '.github/workflows/backend-tests.yml'
  pull_request:
    branches: [ main, dev ]
    paths:
      - 'backend/**'
      - '.github/workflows/backend-tests.yml'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode to run'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - unit-only
          - security-only
          - endpoints-only

env:
  PYTHON_VERSION: "3.12.11"
  
jobs:
  setup-and-discover:
    name: Setup & Endpoint Discovery
    runs-on: ubuntu-latest
    outputs:
      endpoints: ${{ steps.discover.outputs.endpoints }}
      test-files: ${{ steps.discover.outputs.test-files }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-json-report pytest-html
    
    - name: Discover API endpoints
      id: discover
      working-directory: ./backend
      run: |
        # Create endpoint discovery script
        cat > discover_endpoints.py << 'EOF'
        import ast
        import os
        import json
        import re
        from pathlib import Path
        
        def find_fastapi_routes(directory):
            """Discover all FastAPI routes in the codebase"""
            endpoints = []
            
            for py_file in Path(directory).rglob("*.py"):
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Find FastAPI route decorators
                    route_patterns = [
                        r'@router\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']',
                        r'@app\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']'
                    ]
                    
                    for pattern in route_patterns:
                        matches = re.findall(pattern, content, re.MULTILINE)
                        for method, path in matches:
                            # Clean up path parameters
                            clean_path = re.sub(r'\{[^}]+\}', 'test_id', path)
                            endpoints.append({
                                'method': method.upper(),
                                'path': path,
                                'clean_path': clean_path,
                                'file': str(py_file),
                                'needs_auth': 'auth' in path.lower() or 'protected' in content.lower(),
                                'needs_params': '{' in path
                            })
                
                except Exception as e:
                    print(f"Error processing {py_file}: {e}")
            
            return endpoints
        
        def find_test_files(directory):
            """Find all test files"""
            test_files = []
            for py_file in Path(directory).rglob("test_*.py"):
                test_files.append(str(py_file))
            for py_file in Path(directory).rglob("*_test.py"):
                test_files.append(str(py_file))
            return test_files
        
        # Discover endpoints and tests
        endpoints = find_fastapi_routes("domains")
        test_files = find_test_files("tests")
        
        print("Discovered endpoints:")
        for ep in endpoints:
            print(f"  {ep['method']} {ep['path']}")
        
        print(f"\nFound {len(test_files)} test files")
        
        # Output for GitHub Actions
        import os
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"endpoints={json.dumps(endpoints)}\n")
            f.write(f"test-files={json.dumps(test_files)}\n")
        EOF
        
        python discover_endpoints.py
    
    - name: Upload discovery results
      uses: actions/upload-artifact@v4
      with:
        name: endpoint-discovery
        path: |
          backend/discover_endpoints.py

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: setup-and-discover
    if: github.event.inputs.test_mode != 'security-only' && github.event.inputs.test_mode != 'endpoints-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libmagic1 \
          libmagic-dev \
          tesseract-ocr \
          tesseract-ocr-deu \
          tesseract-ocr-eng \
          libtesseract-dev
    
    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install Python dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-json-report pytest-html pytest-cov pytest-xdist
    
    - name: Set up test environment
      working-directory: ./backend
      run: |
        # Create test environment file
        cat > .env.test << EOF
        # Test Environment Configuration
        ENVIRONMENT=test
        DEBUG=False
        
        # Supabase Configuration (from GitHub Secrets)
        SUPABASE_URL=${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY=${{ secrets.SUPABASE_KEY }}
        
        # Security Settings (from GitHub Secrets)
        JWT_SECRET=${{ secrets.JWT_SECRET }}
        JWT_ALGORITHM=${{ secrets.JWT_ALGORITHM }}
        JWT_EXPIRATION=${{ secrets.JWT_EXPIRATION }}
        
        # Test User Credentials
        TEST_EMAIL=krijajannis@gmail.com
        TEST_PASSWORD=221224
        
        # OCR Test Configuration
        OCR_TEST_MOCK_MODE=false
        OCR_TEST_INTEGRATION=true
        
        # Performance Test Limits (CI-friendly)
        OCR_TEST_MAX_AVG_LATENCY_MS=60000
        OCR_TEST_MAX_E2E_AVG_MS=90000
        OCR_TEST_MAX_E2E_MAX_MS=120000
        OCR_TEST_MIN_THROUGHPUT_TPS=0.02
        
        # Database Configuration (Read-only mode)
        DB_READ_ONLY=true
        EOF
    
    - name: Run comprehensive unit tests
      working-directory: ./backend
      run: |
        # Run tests with comprehensive coverage
        pytest tests/ \
          --verbose \
          --tb=short \
          --strict-markers \
          --strict-config \
          --cov=domains \
          --cov=core \
          --cov=middleware \
          --cov=shared \
          --cov-report=xml:coverage.xml \
          --cov-report=html:htmlcov \
          --cov-report=term-missing \
          --cov-fail-under=80 \
          --junitxml=test-results.xml \
          --json-report --json-report-file=test-report.json \
          --html=test-report.html --self-contained-html \
          -x \
          --maxfail=5 \
          --durations=10
    
    - name: Generate test summary
      if: always()
      working-directory: ./backend
      run: |
        echo "## Test Results Summary" > test-summary.md
        echo "" >> test-summary.md
        
        if [ -f test-report.json ]; then
          python3 << 'PYTHON_SCRIPT' >> test-summary.md
import json
with open('test-report.json') as f:
    data = json.load(f)
    
summary = data.get('summary', {})
print(f'- **Total Tests**: {summary.get("total", 0)}')
print(f'- **Passed**: {summary.get("passed", 0)}')
print(f'- **Failed**: {summary.get("failed", 0)}')
print(f'- **Skipped**: {summary.get("skipped", 0)}')
print(f'- **Duration**: {summary.get("duration", 0):.2f}s')
PYTHON_SCRIPT
        fi
        
        echo "" >> test-summary.md
        echo "### Coverage Report" >> test-summary.md
        if [ -f coverage.xml ]; then
          python3 << 'PYTHON_SCRIPT' >> test-summary.md
import xml.etree.ElementTree as ET
tree = ET.parse('coverage.xml')
root = tree.getroot()
coverage = root.attrib.get('line-rate', '0')
print(f'- **Line Coverage**: {float(coverage)*100:.1f}%')
PYTHON_SCRIPT
        fi
        
        cat test-summary.md
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results
        path: |
          backend/test-results.xml
          backend/test-report.json
          backend/test-report.html
          backend/coverage.xml
          backend/htmlcov/
          backend/test-summary.md

  endpoint-tests:
    name: API Endpoint Tests
    runs-on: ubuntu-latest
    needs: setup-and-discover
    if: github.event.inputs.test_mode != 'unit-only' && github.event.inputs.test_mode != 'security-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install httpx pytest-asyncio
    
    - name: Create comprehensive endpoint tests
      working-directory: ./backend
      run: |
        # Create dynamic endpoint test generator
        cat > test_all_endpoints.py << 'EOF'
        import asyncio
        import json
        import pytest
        import httpx
        from fastapi.testclient import TestClient
        from main import app
        
        # Load discovered endpoints
        ENDPOINTS = ${{ needs.setup-and-discover.outputs.endpoints }}
        
        class TestAllEndpoints:
            """Comprehensive tests for all discovered API endpoints"""
            
            @pytest.fixture(scope="class")
            def client(self):
                return TestClient(app)
            
            @pytest.fixture(scope="class")
            def auth_headers(self, client):
                """Get authentication headers for protected endpoints"""
                # Login with test credentials
                login_data = {
                    "email": "krijajannis@gmail.com",
                    "password": "221224"
                }
                
                # Try to find login endpoint
                login_response = None
                for endpoint in ENDPOINTS:
                    if endpoint['method'] == 'POST' and 'login' in endpoint['path'].lower():
                        try:
                            login_response = client.post(endpoint['path'], json=login_data)
                            if login_response.status_code == 200:
                                token = login_response.json().get('access_token')
                                if token:
                                    return {"Authorization": f"Bearer {token}"}
                        except:
                            continue
                
                return {}
            
            @pytest.mark.parametrize("endpoint", ENDPOINTS)
            def test_endpoint_accessibility(self, client, auth_headers, endpoint):
                """Test that all endpoints are accessible and return valid responses"""
                method = endpoint['method']
                path = endpoint['clean_path']
                needs_auth = endpoint['needs_auth']
                
                # Skip password reset endpoints as they require email sending
                if any(skip_term in path.lower() for skip_term in ['reset', 'forgot', 'password-reset']):
                    pytest.skip(f"Skipping password reset endpoint: {method} {path}")
                
                headers = auth_headers if needs_auth else {}
                
                try:
                    if method == 'GET':
                        response = client.get(path, headers=headers)
                    elif method == 'POST':
                        # For POST endpoints, try with minimal valid data
                        test_data = self._get_test_data_for_endpoint(endpoint)
                        response = client.post(path, json=test_data, headers=headers)
                    elif method == 'PUT':
                        test_data = self._get_test_data_for_endpoint(endpoint)
                        response = client.put(path, json=test_data, headers=headers)
                    elif method == 'DELETE':
                        response = client.delete(path, headers=headers)
                    elif method == 'PATCH':
                        test_data = self._get_test_data_for_endpoint(endpoint)
                        response = client.patch(path, json=test_data, headers=headers)
                    else:
                        pytest.skip(f"Unsupported method: {method}")
                    
                    # Check that endpoint responds (not necessarily success)
                    assert response.status_code != 404, f"Endpoint {method} {path} not found"
                    assert response.status_code != 500, f"Endpoint {method} {path} has server error"
                    
                    # Check response format
                    if response.status_code < 400:
                        # Should return valid JSON for API endpoints
                        try:
                            response.json()
                        except:
                            # Non-JSON response might be OK for some endpoints
                            pass
                    
                except Exception as e:
                    pytest.fail(f"Endpoint {method} {path} failed: {str(e)}")
            
            def _get_test_data_for_endpoint(self, endpoint):
                """Generate minimal test data based on endpoint path"""
                path = endpoint['path'].lower()
                
                if 'auth' in path or 'login' in path:
                    return {"email": "test@test.com", "password": "testpass"}
                elif 'user' in path:
                    return {"name": "Test User", "email": "test@test.com"}
                elif 'ingredient' in path:
                    return {"name": "Test Ingredient", "category": "test"}
                elif 'recipe' in path:
                    return {"title": "Test Recipe", "ingredients": []}
                elif 'ocr' in path:
                    return {"image_data": "base64encodedtestdata"}
                else:
                    return {}
            
            @pytest.mark.parametrize("endpoint", [ep for ep in ENDPOINTS if ep['method'] == 'GET' and not any(skip_term in ep['path'].lower() for skip_term in ['reset', 'forgot', 'password-reset'])])
            def test_get_endpoint_performance(self, client, auth_headers, endpoint):
                """Test GET endpoint response times"""
                import time
                
                path = endpoint['clean_path']
                headers = auth_headers if endpoint['needs_auth'] else {}
                
                start_time = time.time()
                response = client.get(path, headers=headers)
                end_time = time.time()
                
                response_time = (end_time - start_time) * 1000  # Convert to ms
                
                # Performance assertions
                if 'health' in path or 'status' in path:
                    assert response_time < 100, f"Health endpoint {path} too slow: {response_time:.2f}ms"
                elif 'ocr' in path:
                    assert response_time < 5000, f"OCR endpoint {path} too slow: {response_time:.2f}ms"
                else:
                    assert response_time < 1000, f"Endpoint {path} too slow: {response_time:.2f}ms"
            
            def test_endpoint_security_headers(self, client):
                """Test that endpoints return appropriate security headers"""
                response = client.get("/health")  # Test with health endpoint
                
                # Check for security headers
                headers = response.headers
                
                # These headers improve security
                security_headers = [
                    'x-content-type-options',
                    'x-frame-options',
                    'x-xss-protection'
                ]
                
                for header in security_headers:
                    assert header in headers.keys() or header.replace('-', '_') in headers.keys(), \
                        f"Missing security header: {header}"
        
        EOF
    
    - name: Run endpoint tests
      working-directory: ./backend
      run: |
        pytest test_all_endpoints.py \
          --verbose \
          --tb=short \
          --junitxml=endpoint-test-results.xml \
          --json-report --json-report-file=endpoint-test-report.json \
          --durations=10
    
    - name: Upload endpoint test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: endpoint-test-results
        path: |
          backend/endpoint-test-results.xml
          backend/endpoint-test-report.json
          backend/test_all_endpoints.py

  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_mode != 'unit-only' && github.event.inputs.test_mode != 'endpoints-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml] safety semgrep pip-audit
    
    - name: Run Bandit security scan
      working-directory: ./backend
      run: |
        bandit -r domains/ core/ middleware/ shared/ \
          -f json -o bandit-report.json \
          -f txt -o bandit-report.txt \
          --severity-level medium \
          --skip B101,B601  # Skip assert and shell injection in tests
        
        echo "## Bandit Security Scan Results" > security-summary.md
        echo "" >> security-summary.md
        cat bandit-report.txt >> security-summary.md
    
    - name: Run Safety dependency scan
      working-directory: ./backend
      run: |
        safety check --json --output safety-report.json || true
        safety check --output safety-report.txt || true
        
        echo "" >> security-summary.md
        echo "## Safety Dependency Scan Results" >> security-summary.md
        echo "" >> security-summary.md
        cat safety-report.txt >> security-summary.md
    
    - name: Run pip-audit for dependency vulnerabilities
      working-directory: ./backend
      run: |
        pip-audit --format=json --output=pip-audit-report.json || true
        pip-audit --format=cyclonedx-json --output=sbom.json || true
        
        echo "" >> security-summary.md
        echo "## Pip-Audit Results" >> security-summary.md
        echo "" >> security-summary.md
        pip-audit || echo "See detailed report in artifacts" >> security-summary.md
    
    - name: Run Semgrep security analysis
      working-directory: ./backend
      run: |
        semgrep --config=auto --json --output=semgrep-report.json domains/ core/ middleware/ shared/ || true
        
        echo "" >> security-summary.md
        echo "## Semgrep Analysis Results" >> security-summary.md
        echo "" >> security-summary.md
        semgrep --config=auto domains/ core/ middleware/ shared/ || echo "See detailed report in artifacts" >> security-summary.md
    
    - name: Check for hardcoded secrets
      working-directory: ./backend
      run: |
        echo "" >> security-summary.md
        echo "## Secret Detection Results" >> security-summary.md
        echo "" >> security-summary.md
        
        # Check for potential secrets
        grep -r -i "password\|secret\|key\|token" --include="*.py" . | \
          grep -v "__pycache__" | \
          grep -v ".env" | \
          head -20 >> security-summary.md || echo "No hardcoded secrets found" >> security-summary.md
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-scan-results
        path: |
          backend/bandit-report.json
          backend/bandit-report.txt
          backend/safety-report.json
          backend/safety-report.txt
          backend/pip-audit-report.json
          backend/sbom.json
          backend/semgrep-report.json
          backend/security-summary.md

  docker-build-test:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: [unit-tests]
    if: github.event.inputs.test_mode != 'unit-only' && github.event.inputs.test_mode != 'endpoints-only' && github.event.inputs.test_mode != 'security-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v4
    
    - name: Build Docker image
      working-directory: ./backend
      run: |
        docker build -t cookify-backend:test .
    
    - name: Test Docker container
      working-directory: ./backend
      run: |
        # Start container in background
        docker run -d --name test-container \
          -e ENVIRONMENT=test \
          -e OCR_TEST_MOCK_MODE=true \
          -p 8000:8000 \
          cookify-backend:test
        
        # Wait for container to be ready
        echo "Waiting for container to start..."
        sleep 15
        
        # Test health endpoint
        echo "Testing health endpoint..."
        curl -f http://localhost:8000/health || exit 1
        
        # Test that container is responding
        echo "Testing API response..."
        curl -f http://localhost:8000/docs || echo "Docs endpoint not available"
        
        # Check container logs
        echo "Container logs:"
        docker logs test-container
        
        # Clean up
        docker stop test-container
        docker rm test-container
    
    - name: Upload Docker test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: docker-test-results
        path: |
          backend/docker-test-logs.txt

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, endpoint-tests, security-tests, docker-build-test]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
    
    - name: Generate comprehensive test report
      run: |
        echo "# ðŸ§ª Cookify Backend Test Report" > final-report.md
        echo "" >> final-report.md
        echo "**Run ID**: ${{ github.run_id }}" >> final-report.md
        echo "**Commit**: ${{ github.sha }}" >> final-report.md
        echo "**Branch**: ${{ github.ref_name }}" >> final-report.md
        echo "**Triggered by**: ${{ github.event_name }}" >> final-report.md
        echo "" >> final-report.md
        
        # Check job statuses
        UNIT_TESTS="${{ needs.unit-tests.result }}"
        ENDPOINT_TESTS="${{ needs.endpoint-tests.result }}"
        SECURITY_TESTS="${{ needs.security-tests.result }}"
        DOCKER_TESTS="${{ needs.docker-build-test.result }}"
        
        echo "## ðŸ“Š Test Results Overview" >> final-report.md
        echo "" >> final-report.md
        echo "| Test Suite | Status |" >> final-report.md
        echo "|------------|--------|" >> final-report.md
        echo "| Unit Tests | $UNIT_TESTS |" >> final-report.md
        echo "| Endpoint Tests | $ENDPOINT_TESTS |" >> final-report.md
        echo "| Security Tests | $SECURITY_TESTS |" >> final-report.md
        echo "| Docker Tests | $DOCKER_TESTS |" >> final-report.md
        echo "" >> final-report.md
        
        # Add detailed results if artifacts exist
        if [ -d "unit-test-results" ]; then
          echo "## ðŸ”¬ Unit Test Details" >> final-report.md
          echo "" >> final-report.md
          if [ -f "unit-test-results/test-summary.md" ]; then
            cat unit-test-results/test-summary.md >> final-report.md
          fi
          echo "" >> final-report.md
        fi
        
        if [ -d "security-scan-results" ]; then
          echo "## ðŸ”’ Security Scan Summary" >> final-report.md
          echo "" >> final-report.md
          if [ -f "security-scan-results/security-summary.md" ]; then
            head -50 security-scan-results/security-summary.md >> final-report.md
          fi
          echo "" >> final-report.md
        fi
        
        # Determine overall status
        OVERALL_STATUS="âœ… PASSED"
        if [[ "$UNIT_TESTS" == "failure" || "$ENDPOINT_TESTS" == "failure" || "$SECURITY_TESTS" == "failure" || "$DOCKER_TESTS" == "failure" ]]; then
          OVERALL_STATUS="âŒ FAILED"
        fi
        
        echo "## ðŸŽ¯ Overall Status: $OVERALL_STATUS" >> final-report.md
        
        cat final-report.md
    
    - name: Comment on PR (if applicable)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('final-report.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: report
          });
    
    - name: Upload final report
      uses: actions/upload-artifact@v4
      with:
        name: final-test-report
        path: final-report.md
