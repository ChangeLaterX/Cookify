name: Backend API Tests

on:
  push:
    branches: [ main, dev ]
    paths:
      - 'backend/**'
      - '.github/workflows/backend-tests.yml'
  pull_request:
    branches: [ main, dev ]
    paths:
      - 'backend/**'
      - '.github/workflows/backend-tests.yml'
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test mode to run'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - unit-only
          - security-only
          - endpoints-only

env:
  PYTHON_VERSION: "3.12.11"
  
jobs:
  setup-and-discover:
    name: Setup & Endpoint Discovery
    runs-on: ubuntu-latest
    outputs:
      endpoints: ${{ steps.discover.outputs.endpoints }}
      test-files: ${{ steps.discover.outputs.test-files }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-json-report pytest-html
    
    - name: Discover API endpoints
      id: discover
      working-directory: ./backend
      run: |
        cat > discover_endpoints.py << 'SCRIPT_END'
import ast
import os
import json
import re
from pathlib import Path

def find_fastapi_routes(directory):
    endpoints = []
    for py_file in Path(directory).rglob("*.py"):
        try:
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            route_patterns = [
                r'@router\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']',
                r'@app\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']'
            ]
            
            for pattern in route_patterns:
                matches = re.findall(pattern, content, re.MULTILINE)
                for method, path in matches:
                    clean_path = re.sub(r'\{[^}]+\}', 'test_id', path)
                    endpoints.append({
                        'method': method.upper(),
                        'path': path,
                        'clean_path': clean_path,
                        'file': str(py_file),
                        'needs_auth': 'auth' in path.lower() or 'protected' in content.lower(),
                        'needs_params': '{' in path
                    })
        except Exception as e:
            print(f"Error processing {py_file}: {e}")
    return endpoints

def find_test_files(directory):
    test_files = []
    for py_file in Path(directory).rglob("test_*.py"):
        test_files.append(str(py_file))
    for py_file in Path(directory).rglob("*_test.py"):
        test_files.append(str(py_file))
    return test_files

endpoints = find_fastapi_routes("domains")
test_files = find_test_files("tests")

print("Discovered endpoints:")
for ep in endpoints:
    print(f"  {ep['method']} {ep['path']}")

print(f"\nFound {len(test_files)} test files")

with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
    f.write(f"endpoints={json.dumps(endpoints)}\n")
    f.write(f"test-files={json.dumps(test_files)}\n")
SCRIPT_END
        
        python discover_endpoints.py
    
    - name: Upload discovery results
      uses: actions/upload-artifact@v4
      with:
        name: endpoint-discovery
        path: backend/discover_endpoints.py

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: setup-and-discover
    if: github.event.inputs.test_mode != 'security-only' && github.event.inputs.test_mode != 'endpoints-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libmagic1 \
          libmagic-dev \
          tesseract-ocr \
          tesseract-ocr-deu \
          tesseract-ocr-eng \
          libtesseract-dev
    
    - name: Cache Python dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install Python dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-json-report pytest-html pytest-cov pytest-xdist
    
    - name: Set up test environment
      working-directory: ./backend
      run: |
        cat > .env.test << 'ENV_END'
ENVIRONMENT=test
DEBUG=False
SUPABASE_URL=${{ secrets.SUPABASE_URL }}
SUPABASE_KEY=${{ secrets.SUPABASE_KEY }}
JWT_SECRET=${{ secrets.JWT_SECRET }}
JWT_ALGORITHM=HS256
JWT_EXPIRATION=1440
TEST_EMAIL=krijajannis@gmail.com
TEST_PASSWORD=221224
OCR_TEST_MOCK_MODE=false
OCR_TEST_INTEGRATION=true
OCR_TEST_MAX_AVG_LATENCY_MS=60000
OCR_TEST_MAX_E2E_AVG_MS=90000
OCR_TEST_MAX_E2E_MAX_MS=120000
OCR_TEST_MIN_THROUGHPUT_TPS=0.02
DB_READ_ONLY=true
ENV_END
    
    - name: Run comprehensive unit tests
      working-directory: ./backend
      run: |
        pytest tests/ \
          --verbose \
          --tb=short \
          --strict-markers \
          --strict-config \
          --cov=domains \
          --cov=core \
          --cov=middleware \
          --cov=shared \
          --cov-report=xml:coverage.xml \
          --cov-report=html:htmlcov \
          --cov-report=term-missing \
          --cov-fail-under=80 \
          --junitxml=test-results.xml \
          --json-report --json-report-file=test-report.json \
          --html=test-report.html --self-contained-html \
          -x \
          --maxfail=5 \
          --durations=10
    
    - name: Generate test summary
      if: always()
      working-directory: ./backend
      run: |
        echo "## Test Results Summary" > test-summary.md
        echo "" >> test-summary.md
        
        if [ -f test-report.json ]; then
          python3 -c "
import json
with open('test-report.json') as f:
    data = json.load(f)
summary = data.get('summary', {})
print('- Total Tests: ' + str(summary.get('total', 0)))
print('- Passed: ' + str(summary.get('passed', 0)))
print('- Failed: ' + str(summary.get('failed', 0)))
print('- Skipped: ' + str(summary.get('skipped', 0)))
print('- Duration: {:.2f}s'.format(summary.get('duration', 0)))
          " >> test-summary.md
        fi
        
        echo "" >> test-summary.md
        echo "### Coverage Report" >> test-summary.md
        if [ -f coverage.xml ]; then
          python3 -c "
import xml.etree.ElementTree as ET
tree = ET.parse('coverage.xml')
root = tree.getroot()
coverage = root.attrib.get('line-rate', '0')
print('- Line Coverage: {:.1f}%'.format(float(coverage)*100))
          " >> test-summary.md
        fi
        
        cat test-summary.md
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results
        path: |
          backend/test-results.xml
          backend/test-report.json
          backend/test-report.html
          backend/coverage.xml
          backend/htmlcov/
          backend/test-summary.md

  endpoint-tests:
    name: API Endpoint Tests
    runs-on: ubuntu-latest
    needs: setup-and-discover
    if: github.event.inputs.test_mode != 'unit-only' && github.event.inputs.test_mode != 'security-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install httpx pytest-asyncio
    
    - name: Create comprehensive endpoint tests
      working-directory: ./backend
      run: |
        cat > test_all_endpoints.py << 'TEST_END'
import asyncio
import json
import pytest
import httpx
from fastapi.testclient import TestClient
from main import app

ENDPOINTS = ${{ needs.setup-and-discover.outputs.endpoints }}

class TestAllEndpoints:
    
    @pytest.fixture(scope="class")
    def client(self):
        return TestClient(app)
    
    @pytest.fixture(scope="class")
    def auth_headers(self, client):
        login_data = {
            "email": "krijajannis@gmail.com",
            "password": "221224"
        }
        
        for endpoint in ENDPOINTS:
            if endpoint['method'] == 'POST' and 'login' in endpoint['path'].lower():
                try:
                    login_response = client.post(endpoint['path'], json=login_data)
                    if login_response.status_code == 200:
                        token = login_response.json().get('access_token')
                        if token:
                            return {"Authorization": f"Bearer {token}"}
                except:
                    continue
        return {}
    
    @pytest.mark.parametrize("endpoint", ENDPOINTS)
    def test_endpoint_accessibility(self, client, auth_headers, endpoint):
        method = endpoint['method']
        path = endpoint['clean_path']
        needs_auth = endpoint['needs_auth']
        
        if any(skip_term in path.lower() for skip_term in ['reset', 'forgot', 'password-reset']):
            pytest.skip(f"Skipping password reset endpoint: {method} {path}")
        
        headers = auth_headers if needs_auth else {}
        
        try:
            if method == 'GET':
                response = client.get(path, headers=headers)
            elif method == 'POST':
                test_data = self._get_test_data_for_endpoint(endpoint)
                response = client.post(path, json=test_data, headers=headers)
            elif method == 'PUT':
                test_data = self._get_test_data_for_endpoint(endpoint)
                response = client.put(path, json=test_data, headers=headers)
            elif method == 'DELETE':
                response = client.delete(path, headers=headers)
            elif method == 'PATCH':
                test_data = self._get_test_data_for_endpoint(endpoint)
                response = client.patch(path, json=test_data, headers=headers)
            else:
                pytest.skip(f"Unsupported method: {method}")
            
            assert response.status_code != 404, f"Endpoint {method} {path} not found"
            assert response.status_code != 500, f"Endpoint {method} {path} has server error"
            
            if response.status_code < 400:
                try:
                    response.json()
                except:
                    pass
            
        except Exception as e:
            pytest.fail(f"Endpoint {method} {path} failed: {str(e)}")
    
    def _get_test_data_for_endpoint(self, endpoint):
        path = endpoint['path'].lower()
        
        if 'auth' in path or 'login' in path:
            return {"email": "test@test.com", "password": "testpass"}
        elif 'user' in path:
            return {"name": "Test User", "email": "test@test.com"}
        elif 'ingredient' in path:
            return {"name": "Test Ingredient", "category": "test"}
        elif 'recipe' in path:
            return {"title": "Test Recipe", "ingredients": []}
        elif 'ocr' in path:
            return {"image_data": "base64encodedtestdata"}
        else:
            return {}
    
    @pytest.mark.parametrize("endpoint", [ep for ep in ENDPOINTS if ep['method'] == 'GET' and not any(skip_term in ep['path'].lower() for skip_term in ['reset', 'forgot', 'password-reset'])])
    def test_get_endpoint_performance(self, client, auth_headers, endpoint):
        import time
        
        path = endpoint['clean_path']
        headers = auth_headers if endpoint['needs_auth'] else {}
        
        start_time = time.time()
        response = client.get(path, headers=headers)
        end_time = time.time()
        
        response_time = (end_time - start_time) * 1000
        
        if 'health' in path or 'status' in path:
            assert response_time < 100, f"Health endpoint {path} too slow: {response_time:.2f}ms"
        elif 'ocr' in path:
            assert response_time < 5000, f"OCR endpoint {path} too slow: {response_time:.2f}ms"
        else:
            assert response_time < 1000, f"Endpoint {path} too slow: {response_time:.2f}ms"
    
    def test_endpoint_security_headers(self, client):
        response = client.get("/health")
        headers = response.headers
        
        security_headers = [
            'x-content-type-options',
            'x-frame-options',
            'x-xss-protection'
        ]
        
        for header in security_headers:
            assert header in headers.keys() or header.replace('-', '_') in headers.keys(), \
                f"Missing security header: {header}"

TEST_END
    
    - name: Run endpoint tests
      working-directory: ./backend
      run: |
        pytest test_all_endpoints.py \
          --verbose \
          --tb=short \
          --junitxml=endpoint-test-results.xml \
          --json-report --json-report-file=endpoint-test-report.json \
          --durations=10
    
    - name: Upload endpoint test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: endpoint-test-results
        path: |
          backend/endpoint-test-results.xml
          backend/endpoint-test-report.json
          backend/test_all_endpoints.py

  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_mode != 'unit-only' && github.event.inputs.test_mode != 'endpoints-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml] safety semgrep pip-audit
    
    - name: Run Bandit security scan
      working-directory: ./backend
      run: |
        bandit -r domains/ core/ middleware/ shared/ \
          -f json -o bandit-report.json \
          -f txt -o bandit-report.txt \
          --severity-level medium \
          --skip B101,B601 || true
        
        echo "## Bandit Security Scan Results" > security-summary.md
        echo "" >> security-summary.md
        if [ -f bandit-report.txt ]; then
          cat bandit-report.txt >> security-summary.md
        fi
    
    - name: Run Safety dependency scan
      working-directory: ./backend
      run: |
        safety check --json --output safety-report.json || true
        safety check --output safety-report.txt || true
        
        echo "" >> security-summary.md
        echo "## Safety Dependency Scan Results" >> security-summary.md
        echo "" >> security-summary.md
        if [ -f safety-report.txt ]; then
          cat safety-report.txt >> security-summary.md
        fi
    
    - name: Run pip-audit for dependency vulnerabilities
      working-directory: ./backend
      run: |
        pip-audit --format=json --output=pip-audit-report.json || true
        pip-audit --format=cyclonedx-json --output=sbom.json || true
        
        echo "" >> security-summary.md
        echo "## Pip-Audit Results" >> security-summary.md
        echo "" >> security-summary.md
        pip-audit || echo "See detailed report in artifacts" >> security-summary.md
    
    - name: Run Semgrep security analysis
      working-directory: ./backend
      run: |
        semgrep --config=auto --json --output=semgrep-report.json domains/ core/ middleware/ shared/ || true
        
        echo "" >> security-summary.md
        echo "## Semgrep Analysis Results" >> security-summary.md
        echo "" >> security-summary.md
        semgrep --config=auto domains/ core/ middleware/ shared/ || echo "See detailed report in artifacts" >> security-summary.md
    
    - name: Check for hardcoded secrets
      working-directory: ./backend
      run: |
        echo "" >> security-summary.md
        echo "## Secret Detection Results" >> security-summary.md
        echo "" >> security-summary.md
        
        grep -r -i "password\|secret\|key\|token" --include="*.py" . | \
          grep -v "__pycache__" | \
          grep -v ".env" | \
          head -20 >> security-summary.md || echo "No hardcoded secrets found" >> security-summary.md
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-scan-results
        path: |
          backend/bandit-report.json
          backend/bandit-report.txt
          backend/safety-report.json
          backend/safety-report.txt
          backend/pip-audit-report.json
          backend/sbom.json
          backend/semgrep-report.json
          backend/security-summary.md

  docker-build-test:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: [unit-tests]
    if: github.event.inputs.test_mode != 'unit-only' && github.event.inputs.test_mode != 'endpoints-only' && github.event.inputs.test_mode != 'security-only'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v4
    
    - name: Build Docker image
      working-directory: ./backend
      run: |
        docker build -t cookify-backend:test .
    
    - name: Test Docker container
      working-directory: ./backend
      run: |
        docker run -d --name test-container \
          -e ENVIRONMENT=test \
          -e OCR_TEST_MOCK_MODE=true \
          -p 8000:8000 \
          cookify-backend:test
        
        echo "Waiting for container to start..."
        sleep 15
        
        echo "Testing health endpoint..."
        curl -f http://localhost:8000/health || exit 1
        
        echo "Testing API response..."
        curl -f http://localhost:8000/docs || echo "Docs endpoint not available"
        
        echo "Container logs:"
        docker logs test-container
        
        docker stop test-container
        docker rm test-container
    
    - name: Upload Docker test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: docker-test-results
        path: backend/docker-test-logs.txt

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, endpoint-tests, security-tests, docker-build-test]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4
    
    - name: Generate comprehensive test report
      run: |
        echo "# Cookify Backend Test Report" > final-report.md
        echo "" >> final-report.md
        echo "**Run ID**: ${{ github.run_id }}" >> final-report.md
        echo "**Commit**: ${{ github.sha }}" >> final-report.md
        echo "**Branch**: ${{ github.ref_name }}" >> final-report.md
        echo "**Triggered by**: ${{ github.event_name }}" >> final-report.md
        echo "" >> final-report.md
        
        UNIT_TESTS="${{ needs.unit-tests.result }}"
        ENDPOINT_TESTS="${{ needs.endpoint-tests.result }}"
        SECURITY_TESTS="${{ needs.security-tests.result }}"
        DOCKER_TESTS="${{ needs.docker-build-test.result }}"
        
        echo "## Test Results Overview" >> final-report.md
        echo "" >> final-report.md
        echo "| Test Suite | Status |" >> final-report.md
        echo "|------------|--------|" >> final-report.md
        echo "| Unit Tests | $UNIT_TESTS |" >> final-report.md
        echo "| Endpoint Tests | $ENDPOINT_TESTS |" >> final-report.md
        echo "| Security Tests | $SECURITY_TESTS |" >> final-report.md
        echo "| Docker Tests | $DOCKER_TESTS |" >> final-report.md
        echo "" >> final-report.md
        
        if [ -d "unit-test-results" ]; then
          echo "## Unit Test Details" >> final-report.md
          echo "" >> final-report.md
          if [ -f "unit-test-results/test-summary.md" ]; then
            cat unit-test-results/test-summary.md >> final-report.md
          fi
          echo "" >> final-report.md
        fi
        
        if [ -d "security-scan-results" ]; then
          echo "## Security Scan Summary" >> final-report.md
          echo "" >> final-report.md
          if [ -f "security-scan-results/security-summary.md" ]; then
            head -50 security-scan-results/security-summary.md >> final-report.md
          fi
          echo "" >> final-report.md
        fi
        
        OVERALL_STATUS="PASSED"
        if [[ "$UNIT_TESTS" == "failure" || "$ENDPOINT_TESTS" == "failure" || "$SECURITY_TESTS" == "failure" || "$DOCKER_TESTS" == "failure" ]]; then
          OVERALL_STATUS="FAILED"
        fi
        
        echo "## Overall Status: $OVERALL_STATUS" >> final-report.md
        
        cat final-report.md
    
    - name: Comment on PR (if applicable)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('final-report.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: report
          });
    
    - name: Upload final report
      uses: actions/upload-artifact@v4
      with:
        name: final-test-report
        path: final-report.md
